{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhattarai-aavash/Pneumonia_Classifier/blob/main/Trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Dataset**\n",
        "\n",
        "The dataset used in this tutorial can be found here. [link](https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia)"
      ],
      "metadata": {
        "id": "XG2hOH0rvTom"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDJuEcIQQmg5",
        "outputId": "26d768d4-d1ce-4202-9262-bfa8ad7f1a82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing Dependencies**"
      ],
      "metadata": {
        "id": "oRNH2rHivSd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models, transforms, datasets"
      ],
      "metadata": {
        "id": "fFSGWVHORJRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create data tansformer**"
      ],
      "metadata": {
        "id": "DLNcLFhbv9gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}"
      ],
      "metadata": {
        "id": "NjEXjXnqB4oV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir ='/content/drive/MyDrive/chest_xray'"
      ],
      "metadata": {
        "id": "9RUX4rG7CFCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_datasets = {\n",
        "    'train': datasets.ImageFolder(root=data_dir+'/train', transform=data_transforms['train']),\n",
        "    'val': datasets.ImageFolder(root=data_dir+'/val', transform=data_transforms['val']),\n",
        "    'test': datasets.ImageFolder(root=data_dir+'/test', transform=data_transforms['test'])\n",
        "}\n"
      ],
      "metadata": {
        "id": "10j2LEw1CQVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(image_datasets['train'].classes))  # Number of classes\n",
        "print(image_datasets['train'].classes)  # Class names\n",
        "sample_image, sample_label = image_datasets['train'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRG0XNCXCS-F",
        "outputId": "fcff258f-6d4e-4ea6-a6e3-589436b29d76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "['NORMAL', 'PNEUMONIA']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create Dataloaders**"
      ],
      "metadata": {
        "id": "q4IZbloWwFXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(image_datasets['train'], batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "val_loader = DataLoader(image_datasets['val'], batch_size=batch_size, shuffle=False, num_workers=1)\n",
        "test_loader = DataLoader(image_datasets['test'], batch_size=batch_size, shuffle=False, num_workers=1)\n"
      ],
      "metadata": {
        "id": "tiuRfeJbCwNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training Model**"
      ],
      "metadata": {
        "id": "ANQCMHKAwLSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.models import resnet50\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Load the pre-trained ResNet-50 model\n",
        "model = resnet50(pretrained=True)\n",
        "\n",
        "# Modify the last fully connected layer to match the number of classes\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Linear(num_features, len(image_datasets['train'].classes))\n",
        "\n",
        "# Set the device for training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Set the path for saving and loading the checkpoint\n",
        "checkpoint_path = '/content/drive/MyDrive/checkpoint.pth'\n",
        "\n",
        "# Check if a checkpoint exists\n",
        "if os.path.exists(checkpoint_path):\n",
        "    # Load the checkpoint\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch']\n",
        "    epoch_loss = checkpoint['loss']\n",
        "else:\n",
        "    start_epoch = 0\n",
        "    epoch_loss = []\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 12\n",
        "\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "\n",
        "    with tqdm(train_loader, unit=\"batch\") as t:\n",
        "        for inputs, labels in t:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            train_correct += torch.sum(preds == labels.data)\n",
        "\n",
        "            t.set_postfix({\"loss\": loss.item()})\n",
        "\n",
        "    train_loss = train_loss / len(image_datasets['train'])\n",
        "    train_acc = train_correct.double() / len(image_datasets['train'])\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            val_correct += torch.sum(preds == labels.data)\n",
        "\n",
        "    val_loss = val_loss / len(image_datasets['val'])\n",
        "    val_acc = val_correct.double() / len(image_datasets['val'])\n",
        "\n",
        "    # Print epoch statistics\n",
        "    print('Epoch {}/{} - Train Loss: {:.4f} - Train Acc: {:.4f} - Val Loss: {:.4f} - Val Acc: {:.4f}'\n",
        "          .format(epoch + 1, num_epochs, train_loss, train_acc, val_loss, val_acc))\n",
        "\n",
        "    # Store epoch loss for tracking\n",
        "    epoch_loss.append(train_loss)\n",
        "\n",
        "    # Save the checkpoint\n",
        "    torch.save({\n",
        "        'epoch': epoch + 1,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': epoch_loss\n",
        "    }, checkpoint_path)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtZvzpTmCbdX",
        "outputId": "707fdfbd-73d8-4f52-f1ac-c340e461025e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 163/163 [01:55<00:00,  1.41batch/s, loss=0.553]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/12 - Train Loss: 0.1113 - Train Acc: 0.9615 - Val Loss: 0.5295 - Val Acc: 0.8125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 163/163 [01:56<00:00,  1.40batch/s, loss=0.233]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/12 - Train Loss: 0.0997 - Train Acc: 0.9630 - Val Loss: 0.4677 - Val Acc: 0.8125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 163/163 [01:55<00:00,  1.42batch/s, loss=0.0338]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/12 - Train Loss: 0.0901 - Train Acc: 0.9651 - Val Loss: 0.3115 - Val Acc: 0.8750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 163/163 [01:55<00:00,  1.41batch/s, loss=0.105]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/12 - Train Loss: 0.0936 - Train Acc: 0.9634 - Val Loss: 0.1756 - Val Acc: 0.9375\n"
          ]
        }
      ]
    }
  ]
}